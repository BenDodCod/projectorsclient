---
phase: 02-validation-i18n
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/benchmark/__init__.py
  - tests/benchmark/test_startup_performance.py
  - tests/benchmark/test_command_performance.py
  - tests/benchmark/test_memory_performance.py
  - tests/benchmark/conftest.py
  - docs/performance/BENCHMARK_RESULTS.md
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Startup time is measured and documented"
    - "Command execution time is measured and documented"
    - "Memory usage is measured and documented"
    - "Benchmarks can be run via pytest"
    - "Results are stored for trend tracking"
  artifacts:
    - path: "tests/benchmark/test_startup_performance.py"
      provides: "Startup time benchmark tests"
      min_lines: 60
    - path: "tests/benchmark/test_command_performance.py"
      provides: "Command execution benchmark tests"
      min_lines: 60
    - path: "tests/benchmark/test_memory_performance.py"
      provides: "Memory usage benchmark tests"
      min_lines: 60
    - path: "docs/performance/BENCHMARK_RESULTS.md"
      provides: "Documented benchmark results"
      contains: "Startup Time"
  key_links:
    - from: "tests/benchmark/test_startup_performance.py"
      to: "src/main.py"
      via: "time.perf_counter around app init"
      pattern: "perf_counter"
    - from: "tests/benchmark/test_memory_performance.py"
      to: "psutil.Process"
      via: "memory_info().rss"
      pattern: "memory_info"
---

<objective>
Create performance benchmark test suite and establish baseline measurements.

Purpose: Verify and document that the application meets performance requirements PERF-04 (startup <2s), PERF-05 (command execution <5s), PERF-06 (memory <150MB). Establish benchmarks for ongoing monitoring.

Output: Benchmark test suite with documented baseline measurements.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/GSD_ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-validation-i18n/02-RESEARCH.md
@src/main.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create benchmark test infrastructure and startup performance tests</name>
  <files>
    - tests/benchmark/__init__.py
    - tests/benchmark/conftest.py
    - tests/benchmark/test_startup_performance.py
  </files>
  <action>
    1. Create tests/benchmark/__init__.py:
       - Module docstring explaining benchmark suite purpose
       - Import benchmark utilities

    2. Create tests/benchmark/conftest.py:
       - Import pytest, time, psutil
       - Create @pytest.fixture(scope="session") for benchmark_results collector
       - Create @pytest.fixture for fresh_qapplication (isolated Qt app for each test)
       - Create @pytest.fixture for mock_database (in-memory SQLite for benchmarks)
       - Add pytest_configure hook to register 'benchmark' marker
       - Add pytest_terminal_summary hook to print benchmark summary

    3. Create tests/benchmark/test_startup_performance.py:
       - Import time, pytest, psutil
       - Import from src.main, src.ui.main_window
       - Class TestStartupPerformance:
         - @pytest.mark.benchmark
         - test_cold_startup_under_2_seconds(qtbot, fresh_qapplication):
           - Measure time from QApplication creation to MainWindow.show()
           - Assert elapsed < 2.0 with message showing actual time
           - Record to benchmark_results fixture
         - test_warm_startup_under_1_second(qtbot):
           - Import all modules first (warm cache)
           - Measure only MainWindow creation
           - Assert elapsed < 1.0
         - test_import_time_breakdown():
           - Use importlib to measure individual module import times
           - Log top 5 slowest imports
           - No assertion, informational only

    4. Add psutil to requirements-dev.txt if not present
  </action>
  <verify>
    - Run: pytest tests/benchmark/test_startup_performance.py -v -s
    - Tests pass and show timing information
  </verify>
  <done>
    - Benchmark infrastructure created
    - Startup tests measure cold and warm startup
    - Tests assert against 2-second target
  </done>
</task>

<task type="auto">
  <name>Task 2: Create command execution and memory usage benchmark tests</name>
  <files>
    - tests/benchmark/test_command_performance.py
    - tests/benchmark/test_memory_performance.py
  </files>
  <action>
    1. Create tests/benchmark/test_command_performance.py:
       - Import pytest, time, socket
       - Import from src.core.projector_controller, src.network.pjlink_protocol
       - Class TestCommandPerformance:
         - @pytest.mark.benchmark
         - test_pjlink_command_encoding_under_1ms():
           - Time PJLinkCommands encoding 1000 times
           - Assert average < 0.001 seconds
         - test_mock_network_roundtrip_under_5_seconds(mock_pjlink_server):
           - Use mock PJLink server from tests/mocks/
           - Send power_on command, measure full roundtrip
           - Assert elapsed < 5.0 seconds
         - test_command_queue_throughput():
           - Queue 10 commands, measure total processing time
           - Assert < 10 seconds (1s per command average)

    2. Create tests/benchmark/test_memory_performance.py:
       - Import pytest, psutil, gc
       - Import from src.main, src.ui.main_window
       - Class TestMemoryPerformance:
         - @pytest.mark.benchmark
         - test_baseline_memory_under_150mb(qtbot, fresh_qapplication):
           - Create MainWindow with mock database
           - Force garbage collection
           - Measure psutil.Process().memory_info().rss
           - Assert memory_mb < 150 with message
         - test_memory_after_100_operations(qtbot):
           - Simulate 100 UI operations (button clicks, updates)
           - Measure memory before and after
           - Assert no significant leak (delta < 10MB)
         - test_translation_manager_memory():
           - Load both en.json and he.json
           - Measure memory used by translations
           - Log for informational purposes

    3. Ensure mock_pjlink_server fixture is available from tests/mocks/pjlink_server.py
  </action>
  <verify>
    - Run: pytest tests/benchmark/test_command_performance.py tests/benchmark/test_memory_performance.py -v -s
    - Tests pass and show performance measurements
  </verify>
  <done>
    - Command execution tests verify <5s target
    - Memory tests verify <150MB target
    - Memory leak detection for 100 operations
  </done>
</task>

<task type="auto">
  <name>Task 3: Run benchmarks and document results</name>
  <files>
    - docs/performance/BENCHMARK_RESULTS.md
  </files>
  <action>
    1. Create docs/performance/ directory if not exists

    2. Run full benchmark suite:
       - pytest tests/benchmark/ -v -s --tb=short
       - Capture output for documentation

    3. Create docs/performance/BENCHMARK_RESULTS.md:
       - Header: "# Performance Benchmark Results"
       - Date and environment section (Python version, Windows version, hardware)
       - ## Startup Performance
         - Target: <2 seconds
         - Measured: X.XX seconds
         - Status: PASS/FAIL
       - ## Command Execution Performance
         - Target: <5 seconds
         - Measured: X.XX seconds
         - Status: PASS/FAIL
       - ## Memory Performance
         - Target: <150MB
         - Measured: XXX MB
         - Status: PASS/FAIL
       - ## Detailed Results
         - Table of all benchmark results
       - ## Optimization Recommendations
         - List any areas close to limits
       - ## Historical Trends
         - Placeholder for future runs

    4. If any target is NOT met:
       - Document the gap
       - Add OPTIMIZATION_NEEDED.md with specific recommendations
       - Focus areas from research: module imports, translation loading, database init
  </action>
  <verify>
    - File exists: docs/performance/BENCHMARK_RESULTS.md
    - All three targets documented with actual measurements
  </verify>
  <done>
    - BENCHMARK_RESULTS.md contains actual measurements
    - Pass/fail status documented for each target
    - Optimization recommendations provided if needed
  </done>
</task>

</tasks>

<verification>
Run complete benchmark suite:
```bash
pytest tests/benchmark/ -v -s --tb=short
```

Verify documentation:
```bash
cat docs/performance/BENCHMARK_RESULTS.md
```

Expected targets:
- PERF-04: Startup <2 seconds
- PERF-05: Command execution <5 seconds
- PERF-06: Memory <150MB
</verification>

<success_criteria>
- PERF-04: Application startup time measured and documented (<2 seconds target)
- PERF-05: Command execution time measured and documented (<5 seconds target)
- PERF-06: Memory usage measured and documented (<150MB target)
- Benchmark test suite passes (pytest tests/benchmark/ -v)
- Results documented in docs/performance/BENCHMARK_RESULTS.md
</success_criteria>

<output>
After completion, create `.planning/phases/02-validation-i18n/02-02-SUMMARY.md`
</output>
